---
title: Computer Vision aplicaciones para auriculares de realidad mixta Workshop en CVPR 2019
description: Información general y programación de las aplicaciones de Computer Vision para los auriculares con micrófonos de realidad mixta, que se entregarán en la Conferencia de CVPR del 2019 de junio.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: evento, modo de investigación, CVPR, Computer Vision, investigación, HoloLens
ms.openlocfilehash: 55fbeea1f1293c7df5eae489b6504851bf6bca7f
ms.sourcegitcommit: 09599b4034be825e4536eeb9566968afd021d5f3
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 10/03/2020
ms.locfileid: "91693115"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a>Computer Vision aplicaciones para auriculares de realidad mixta

Organizado junto con [CVPR 2019](https://cvpr2019.thecvf.com/)

Larga playa (CA)

17 de junio de 2019 (tarde)-Hyatt Regency F


## <a name="organizers"></a>Los organizadores
* Marc Pollefeys
* Federica Bogo
* Johannes Schönberger
* Osman Ulusoy

## <a name="overview"></a>Información general

![Imagen de rompecabezas](images/cvpr2019_teaser2.jpg)

Los auriculares de realidad mixta, como Microsoft HoloLens, están convirtiéndose en plataformas eficaces para desarrollar aplicaciones para equipos. El modo de investigación de HoloLens habilita la investigación de Computer Vision en el dispositivo proporcionando acceso a todas las secuencias de sensor de imagen sin procesar, incluida la profundidad y el IR. Dado que el modo de investigación ahora está disponible desde el 2018 de mayo, comenzamos a ver varias demostraciones interesantes y aplicaciones que se desarrollan para HoloLens. 

El objetivo de este taller es reunir a los estudiantes e investigadores interesados en Computer Vision para aplicaciones de realidad mixta. El taller proporcionará un lugar para compartir demostraciones y aplicaciones y aprenderá entre sí para compilar o migrar aplicaciones a la realidad mixta. 

Se recomiendan los envíos de los temas de reconocimiento de objetos (centrados en ego), seguimiento de usuarios y mano, reconocimiento de actividades, ABRIREMOS, reconstrucción 3D, comprensión de escenas, localización basada en sensores, navegación, etc.

## <a name="paper-submission"></a>Envío de papel
* Fecha límite de envío de papel: 17 de mayo
* Notificación a los autores: 24 de mayo

Los envíos de papel deben usar la plantilla CVPR y están limitados a 4 páginas más referencias. Además, se recomienda a los autores que envíen un vídeo en el que se muestre su aplicación.
Tenga en cuenta que se permiten los envíos de trabajos publicados anteriormente (incluido el trabajo aceptado en la Conferencia principal de CVPR 2019). 

Los envíos se pueden cargar en el CMT: https://cmt3.research.microsoft.com/CVFORMR2019

Se seleccionará un subconjunto de papeles para la presentación oral en el taller. Sin embargo, se recomienda encarecidamente a todos los autores que presenten su trabajo durante la sesión de demostración.


## <a name="schedule"></a>Programación
* 13:30-13:45: bienvenida y apertura de comentarios.
* 13:45-14:15: **discurso de habla** : Prof. Marc POLLEFEYS, ETH Zurich/Microsoft. Title: Egocentric Computer Vision en HoloLens.
* 14:15-14:45: **discurso de habla** : Prof. Kris Kitani, Carnegie Mellon University. Title: actividad Egocentric y presentar previsión.
* 14:45-15:15: **charla** : Dr. Yang Liu, California Institute of Technology. Título: potenciación de un asistente cognitivo para ciego con realidad aumentada.
* 15:15-16:15: pausa de café y demostraciones.
* 16:15-16:45: **discurso** de la ponencia: Prof. Kristen Grauman, Universidad de Texas en investigación de Austin/Facebook para Ia. Title: interacción del objeto humano en el vídeo de la primera persona.
* 16:45-17:15: presentaciones orales:
    * Registro simplificado: navegación Orthopedic independiente con HoloLens. F. Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. Snedeker, M. Farshad, P. Furnstahl.
    * Aprender estéreo con un HoloLens. H. Zhan, Y. Pekelny, O. Ulusoy.
* 17:15-17:30: Notas finales.
